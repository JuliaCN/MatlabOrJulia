# Machine learning and Automatic differentiation

* [Knet](https://github.com/denizyuret/Knet.jl)
Knet is a traditional machine learning framework with a proper performance. You can use it to build up your ML applications without much worries.
* [ForwardDiff](https://github.com/JuliaDiff/ForwardDiff.jl)
It is a forward mode automatic differentiation for Julia, with a state of the art performance. Check the [paper](https://arxiv.org/abs/1607.07892) for details.
* [NiLang](https://github.com/GiggleLiu/NiLang.jl)
NiLang is a differential reversible eDSL, any program written in this eDSL can be differentiated efficiently. Check the [paper](https://arxiv.org/abs/2003.04617) for details.
* [Flux](https://github.com/FluxML/Flux.jl)
Flux is the most popular machine learning library in Julia. Check the [paper](http://arxiv.org/abs/1811.01457) for detail.
* [ReverseDiff](https://github.com/JuliaDiff/ReverseDiff.jl)
It is a reverse mode autodiff tool that can differentiate a general Julia program with a proper performance.
* [Zygote](https://github.com/FluxML/Zygote.jl)
Zygote is an easy to use autodiff framework based on source code transformation, it can differentiate a general Julia program.
* [Nabla](https://github.com/invenia/Nabla.jl)
* [Yota](https://github.com/dfdx/Yota.jl)

## Gripes
None of the above machine learning packages can be as stable as popular python packages like TensorFlow and Pytorch. Need some try and errors.
